# -*- coding: utf-8 -*-
"""mask2former.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12QfIEGxHdI57FXNSqFU5QjtkaLbggnf3
"""

import requests
import torch
from PIL import Image
from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
import json
import torch
from torch import nn
import matplotlib.pyplot as plt
from matplotlib.path import Path
import numpy as np
import os


# load Mask2Former fine-tuned on Mapillary Vistas semantic segmentation
processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-large-mapillary-vistas-semantic")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-large-mapillary-vistas-semantic")

# image = Image.open("rs19_val/jpgs/rs19_val/" + "rs08220"+".jpg")
# # image = images[0]
# print(image.size)

folder_path = "rs19_val/jpgs/rs19_val"
data_dir = "rs19_val/jpgs/rs19_val"
# images = []

# for filename in os.listdir(folder_path):
#     if filename.endswith(".jpg"):
#         file_path = os.path.join(folder_path, filename)
#         images.append(Image.open(file_path))

# Calculate and extract avaliable json groundtruth
counter = 0
images_available_num = 0
images_available_li = []
def get_all_labels(images_available_li, images_available_num, counter, input_name):
    try:
        with open("rs19_val/jsons/rs19_val/" + input_name + ".json", "r") as json_file:
            ground_truth = json.load(json_file)
    except FileNotFoundError:
        counter += 1
        # print(f"Following file not found in groundtruth {input_name}")
        return counter, images_available_num
    except Exception as e:
        counter += 1
        # print(f"Error occurred while reading {input_name}: {e}")
        return counter, images_available_num

    try:
        with Image.open("rs19_val/jpgs/rs19_val/" + input_name+".jpg") as image:
            images_available_num += 1
    except FileNotFoundError:
        counter += 1
        # print(f"Following file not found in images {input_name}")
        return counter, images_available_num
    except Exception as e:
        counter += 1
        # print(f"Error occurred while image {input_name}: {e}")
        return counter, images_available_num

    images_available_li.append(input_name)

    return counter, images_available_num


folder_path = "rs19_val/jsons/rs19_val"
file_names = []
for filename in os.listdir(folder_path):
    if filename.endswith(".json"):
        file_path = os.path.join(folder_path, filename)
        file_names.append(filename)

all_keys = set()
for name in file_names:
    counter, images_available_num = get_all_labels(images_available_li, images_available_num, counter, name[:-5])

print(f"Unavailable ground truth {counter}")
print(f"Available ground truth {images_available_num}")
# Example of available groundtruth
print(images_available_li[:10])

# Return all types of labels in the rail class
def get_all_labels(counter, input_name):
    # print(input_name)
    try:
        with open("rs19_val/jsons/rs19_val/" + input_name + ".json", "r") as json_file:
            ground_truth = json.load(json_file)
    except Exception as e:
        # counter += 1
        print(f"Error occurred while reading {input_name}: {e}")
        return counter, {}

    all_objects = ground_truth['objects']
    all_points = []
    all_type_labels_keys = set()
    for i in all_objects:
        all_classes.add(i['label'])
        # if i['label'] == 'rail':
        for ind, j in enumerate(i.keys()):
            if ind ==0:
                continue
            # if j == 'polyline' and i['label'] == 'rail':
            #     print(input_name)
            all_type_labels_keys.add(j)

    return counter, all_type_labels_keys


all_keys = set()
all_classes = set()
for name in images_available_li:
    # if name[:-5] == 'rs04203':
    #     continue
    counter, keys = get_all_labels(counter, name)
    all_keys = all_keys.union(keys)

all_keys = all_keys - {'label'}

print("all potential shape type:")
print(all_keys)

print("all potential classes:")
print(all_classes)

# Only remain following categories rail => 12, person => 19, Pedestrian Area => 11, unlabeled => -1
# & Pedestrian Area & Person & unlabeled

# Detect points inside frames
def polygon_coord(points):
    original_points=points

    x_size = 1920
    y_size = 1080

    x, y = np.meshgrid(np.arange(x_size), np.arange(y_size))
    x, y = x.flatten(), y.flatten()
    points = np.vstack((x,y)).T

    p = Path(original_points)
    grid = p.contains_points(points)
    mask = grid.reshape(y_size, x_size)
    matrix = torch.zeros(y_size, x_size)

    points = []
    for ind_r, r in enumerate(mask):
        for ind_c, c in enumerate(r):
            if mask[ind_r, ind_c]:
                points.append((ind_r, ind_c))
    return points

def polygon(points):
    original_points=points

    x_size = 1920
    y_size = 1080

    x, y = np.meshgrid(np.arange(x_size), np.arange(y_size))
    x, y = x.flatten(), y.flatten()
    points = np.vstack((x,y)).T

    p = Path(original_points)
    grid = p.contains_points(points)
    mask = grid.reshape(y_size, x_size)
    matrix = torch.zeros(y_size, x_size)

    for ind_r, r in enumerate(mask):
        for ind_c, c in enumerate(r):
            if mask[ind_r, ind_c]:
                matrix[ind_r, ind_c] = 1

    return matrix


def bounding_box_coord(bounding_box):
    x_min, y_min, x_max, y_max = bounding_box
    points = []
    for x in range(x_min, x_max + 1):
        for y in range(y_min, y_max + 1):
            points.append([x, y])
    return points


def poly_pair_coord(pairs):
    all_points = []
    count = 0
    for i in pairs:
        if count == 1:
            for j in i[::-1]:
                # matrix[j[1],j[0]]= 1
                all_points.append((j[0],j[1]))
        else:
            count += 1
            for j in i:
                # matrix[j[1],j[0]]= 1
                all_points.append((j[0],j[1]))

    return polygon_coord(all_points)

def poly_coord(points):
    all_points = []
    for j in points:
        all_points.append((j[0],j[1]))
    return polygon_coord(all_points)




# points = [[1143, 1079], [980, 507], [979, 510],[865, 1078]]

# matrix = polygon(points)
# # print(matrix)
# plt.figure(figsize=(20, 10))
# plt.imshow(matrix)

# Example of rail coloring
with open("rs19_val/jsons/rs19_val/" + "rs04203" + ".json", "r") as json_file:
    ground_truth = json.load(json_file)
# for ind, i in enumerate(ground_truth['objects']):
#     print(i)
ground_truth['objects'][-1]['polyline']
matrix = torch.zeros(1080, 1920)
all_points = []
count = 0
for j in ground_truth['objects'][-1]['polyline']:
        matrix[j[1],j[0]]= 1
        all_points.append((j[0],j[1]))

# all_points = [ground_truth['objects'][10]['polyline-pair'][0][0], ground_truth['objects'][10]['polyline-pair'][0][-1], ground_truth['objects'][10]['polyline-pair'][1][-1], ground_truth['objects'][10]['polyline-pair'][1][0]]
# all_points = ground_truth['objects'][11]['polyline-pair']

matrix = polygon(all_points)
plt.figure(figsize=(10, 5))
plt.imshow(matrix)

# Example of rail coloring
with open("rs19_val/jsons/rs19_val/" + "rs00957" + ".json", "r") as json_file:
    ground_truth = json.load(json_file)
# ground_truth['objects'][11]['polyline-pair']
matrix = torch.zeros(1080, 1920)
all_points = []
count = 0
for i in ground_truth['objects'][11]['polyline-pair']:
    if count == 1:
        for j in i[::-1]:
            matrix[j[1],j[0]]= 1
            all_points.append((j[0],j[1]))
    else:
        count += 1
        for j in i:
            matrix[j[1],j[0]]= 1
            all_points.append((j[0],j[1]))


#all_points = [ground_truth['objects'][10]['polyline-pair'][0][0], ground_truth['objects'][10]['polyline-pair'][0][-1], ground_truth['objects'][10]['polyline-pair'][1][-1], ground_truth['objects'][10]['polyline-pair'][1][0]]
#all_points = ground_truth['objects'][11]['polyline-pair']

matrix = polygon(all_points)
plt.figure(figsize=(10, 5))
plt.imshow(matrix)

file_path = "labeled.json"

# Read the JSON file
with open(file_path, "r") as json_file:
    ground_truth_dict = json.load(json_file)

# ground_truth_dict

"""## Testing using mask2former"""

# Get ground truth labels in dictionary.
from tqdm import tqdm
ground_truth_dict = {}
for input_name in tqdm(images_available_li):
    json_file = open("rs19_val/jsons/rs19_val/" + input_name + ".json", "r")
    ground_truth = json.load(json_file)
    all_objects = ground_truth['objects']
    all_points_rail = []
    all_points_person = []
    all_points_crossing = []
    for i in all_objects:
        if i['label'] == 'rail':
            if 'polyline-pair' in i.keys():
                all_points_rail += poly_pair_coord(i['polyline-pair'])
            elif 'boundingbox' in i.keys():
                all_points_rail += bounding_box_coord(i['boundingbox'])
            elif 'polyline' in i.keys():
                all_points_rail += poly_coord(i['polyline'])
            elif 'polygon' in i.keys():
                all_points_rail += polygon_coord(i['polygon'])
        elif i['label'] == 'crossing':
            if 'polyline-pair' in i.keys():
                all_points_crossing  += poly_pair_coord(i['polyline-pair'])
            elif 'boundingbox' in i.keys():
                all_points_crossing  += bounding_box_coord(i['boundingbox'])
            elif 'polyline' in i.keys():
                all_points_crossing  += poly_coord(i['polyline'])
            elif 'polygon' in i.keys():
                all_points_crossing  += polygon_coord(i['polygon'])
        elif i['label'] == 'person':
            if 'polyline-pair' in i.keys():
                all_points_person += poly_pair_coord(i['polyline-pair'])
            elif 'boundingbox' in i.keys():
                all_points_person += bounding_box_coord(i['boundingbox'])
            elif 'polyline' in i.keys():
                all_points_person += poly_coord(i['polyline'])
            elif 'polygon' in i.keys():
                all_points_person += polygon_coord(i['polygon'])
    ground_truth_dict[input_name] = {}
    ground_truth_dict[input_name]['Person'] = all_points_person
    ground_truth_dict[input_name]['Crossing'] = all_points_crossing
    ground_truth_dict[input_name]['Rail'] = all_points_rail

def val(input_name):
    image = Image.open("rs19_val/jpgs/rs19_val/" + input_name+".jpg")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)

    class_queries_logits = outputs.class_queries_logits
    masks_queries_logits = outputs.masks_queries_logits

    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

    # rail => 12, person => 19, Pedestrian Area => 11, unlabeled => -1
    ret = {}
    # Rail
    indices = torch.nonzero(predicted_semantic_map == 12, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list = reversed_indices.tolist()
    reversed_indices_list = sorted(reversed_indices_list)
    # indices_set_rail = set(reversed_indices_list)

    all_points_rail = set(ground_truth_dict[input_name]['Rail'])
    cnt = 0
    for i in reversed_indices_list:
        if (i[1], i[0]) in all_points_rail:
            cnt += 1
    image.close()
    iou = cnt / (len(all_points_rail) +len(reversed_indices_list) - cnt)
    ret["Rail"] = iou

    # Person
    indices = torch.nonzero(predicted_semantic_map == 19, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list = reversed_indices.tolist()
    reversed_indices_list = sorted(reversed_indices_list)
    # indices_set_person = set(reversed_indices_list)

    all_points_person = set(ground_truth_dict[input_name]['Person'])
    cnt = 0
    for i in reversed_indices_list:
        if (i[1], i[0]) in all_points_person:
            cnt += 1
    image.close()
    if cnt == 0:
        ret["Person"] = np.NaN
    else:
        iou = cnt / (len(all_points_person) +len(reversed_indices_list) - cnt)
        ret["Person"] = iou

    # Crossing
    indices = torch.nonzero(predicted_semantic_map == 11, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list = reversed_indices.tolist()
    reversed_indices_list = sorted(reversed_indices_list)
    indices = torch.nonzero(predicted_semantic_map == 13, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list2 = reversed_indices.tolist()
    reversed_indices_list2 = sorted(reversed_indices_list2)
    reversed_indices_list = reversed_indices_list + reversed_indices_list2
    # indices_set_crossing = set(reversed_indices_list)

    # print(ground_truth_dict[input_name]['Crossing'])
    all_points = [tuple(lst) for lst in ground_truth_dict[input_name]['Crossing']]
    all_points_crossing = set(all_points)

    cnt = 0
    for i in reversed_indices_list:
        if (i[1], i[0]) in all_points_crossing:
            cnt += 1
    image.close()
    if cnt == 0:
        ret["Crossing"] = np.NaN
    else:
        iou = cnt / (len(all_points_crossing) +len(reversed_indices_list) - cnt)
        ret["Crossing"] = iou

    # Background
    # indices = torch.nonzero(predicted_semantic_map not in [11,12,13,19], as_tuple=False)
    # reversed_indices = indices[:, [1, 0]]
    # reversed_indices_list = reversed_indices.tolist()
    # reversed_indices_list = sorted(reversed_indices_list)
    # indices_set = indices_set_crossing + indices_set_person + indices_set_rail

    # all_points = all_points_crossing + all_points_person + all_points_rail
    # intersect = 0
    # union = len(indices_set)
    # for i in range(1080):
    #     for j in range(1920):
    #         if (i,j) not in all_points:
    #             if (i,j) not in indices_set:
    #                 intersect += 1
    #             else:
    #                 union += 1

    # iou = cnt / (len(all_points) +len(reversed_indices_list) - cnt)
    # ret["Background"] = iou


    return ret

rail_ious = []
for i in list(ground_truth_dict.keys()):
    print('image: ', i)
    ret = val(i)
    print("IOU for Rail class", str(round(ret['Rail'] * 100, 2)) + "%")
    rail_ious.append(ret['Rail'])
    if not np.isnan(ret['Person']):
        print("IOU for Person class", str(round(ret['Person'] * 100, 2)) + "%")
    if not np.isnan(ret['Crossing']):
        print("IOU for Crossing class", str(round(ret['Crossing'] * 100, 2)) + "%")
    # print("IOU for Background", str(round(ret['Background'] * 100, 2)) + "%")
    print("===============")

np.average(np.array(rail_ious))

# Visualization of Rail Class
inputs = ["rs08220", "rs03179", "rs04346"]
for input in inputs:
    image = Image.open("rs19_val/jpgs/rs19_val/" + input+".jpg")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)

    class_queries_logits = outputs.class_queries_logits
    masks_queries_logits = outputs.masks_queries_logits


    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
    rail_indices = torch.nonzero(predicted_semantic_map == 12, as_tuple=False)
    # print(indices.sorted(lambda: x[-1]))
    gt_predicted = torch.zeros_like(predicted_semantic_map)
    gt_predicted[rail_indices[:, 0], rail_indices[:, 1]] = 1

    # Plot the visualization
    plt.figure(figsize=(5, 10))
    plt.subplot(3, 1, 1)
    plt.title('Image')
    plt.imshow(image)

    plt.subplot(3, 1, 2)
    plt.title('Predicted Rail Class')
    plt.imshow(gt_predicted, cmap='gray')


    gt = torch.zeros(1080, 1920)

    for i in ground_truth_dict[input]['Rail']:
        # print(i)
        gt[i] = 1

    plt.subplot(3, 1, 3)
    plt.title('Ground Truth Rail Class')
    plt.imshow(gt, cmap='gray')
    plt.show()

import torch
import matplotlib.pyplot as plt
from PIL import Image

# Assuming the rest of your code is here...

# Visualization of Rail Class
inputs = ["rs08220", "rs03179", "rs04346"]
for input in inputs:
    image = Image.open("rs19_val/jpgs/rs19_val/" + input + ".jpg")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)

    class_queries_logits = outputs.class_queries_logits
    masks_queries_logits = outputs.masks_queries_logits

    # Get the predicted semantic map
    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

    # Create a binary mask for rail class (label 12)
    rail_mask = (predicted_semantic_map == 12).float()

    # Plot the visualization
    plt.figure(figsize=(10, 5))

    # Plot the original image
    plt.subplot(1, 2, 1)
    plt.title('Original Image')
    plt.imshow(image)
    plt.axis('off')

    # Plot the predicted rail class overlayed on the original image
    plt.subplot(1, 2, 2)
    plt.title('Predicted Rail Class Overlay')
    plt.imshow(image)
    plt.imshow(rail_mask, cmap='jet', alpha=0.5)  # Overlay mask on the image with transparency
    plt.axis('off')

    plt.show()



"""## Preprocess image"""

import cv2
import numpy as np

image = cv2.imread("rs19_val/jpgs/rs19_val/" +  "rs03179" +".jpg")

blurred_image = cv2.GaussianBlur(image, (5, 5), 0)

bilateral_filtered_image = cv2.bilateralFilter(blurred_image, 9, 75, 75)

sharpened_image = cv2.filter2D(bilateral_filtered_image, -1, np.array([[-1, -1, -1],
                                                                      [-1,  9, -1],
                                                                      [-1, -1, -1]]))

gray_image = cv2.cvtColor(sharpened_image, cv2.COLOR_BGR2GRAY)
equalized_image = cv2.equalizeHist(gray_image)
enhanced_image = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2BGR)

kernel = np.ones((5, 5), np.uint8)
opening_image = cv2.morphologyEx(enhanced_image, cv2.MORPH_OPEN, kernel)

plt.imshow(blurred_image, cmap='gray')

plt.imshow(bilateral_filtered_image, cmap='gray')

plt.imshow(sharpened_image, cmap='gray')

plt.imshow(opening_image, cmap='gray')

import torch
import matplotlib.pyplot as plt
from PIL import Image

# Assuming the rest of your code is here...

# Visualization of Rail Class
inputs = ["rs08220", "rs03179", "rs04346"]
for input in inputs:
    image = Image.open("rs19_val/jpgs/rs19_val/" + input + ".jpg")
    image_np = np.array(image)

    # blurred_image = cv2.GaussianBlur(image, (5, 5), 0)

    # bilateral_filtered_image = cv2.bilateralFilter(image_np, 9, 75, 75)

    # sharpened_image = cv2.filter2D(bilateral_filtered_image, -1, np.array([[-1, -1, -1],
    #                                                                     [-1,  9, -1],
                                                                        #[-1, -1, -1]]))

    # gray_image = cv2.cvtColor(sharpened_image, cv2.COLOR_BGR2GRAY)
    # equalized_image = cv2.equalizeHist(gray_image)
    # enhanced_image = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2BGR)
    image_float = image_np.astype(float)
    brightness_factor = 5

    brightened_image = cv2.add(image_float, brightness_factor)
    brightened_image = np.clip(brightened_image, 0, 255)
    brightened_image = brightened_image.astype(np.uint8)

    inputs = processor(images=brightened_image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)

    class_queries_logits = outputs.class_queries_logits
    masks_queries_logits = outputs.masks_queries_logits

    # Get the predicted semantic map
    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

    # Create a binary mask for rail class (label 12)
    rail_mask = (predicted_semantic_map == 12).float()

    # Plot the visualization
    plt.figure(figsize=(10, 5))

    # Plot the original image
    plt.subplot(1, 2, 1)
    plt.title('Original Image')
    plt.imshow(image)
    plt.axis('off')

    # Plot the predicted rail class overlayed on the original image
    plt.subplot(1, 2, 2)
    plt.title('Predicted Rail Class Overlay')
    plt.imshow(image)
    plt.imshow(rail_mask, cmap='jet', alpha=0.5)  # Overlay mask on the image with transparency
    plt.axis('off')

    plt.show()



from PIL import ImageFilter
import cv2
def val_blurred(input_name):
    image = Image.open("rs19_val/jpgs/rs19_val/" + input_name+".jpg")
    image_np = np.array(image)

    # blurred_image = cv2.GaussianBlur(image_np, (7, 7), 0)

    bilateral_filtered_image = cv2.bilateralFilter(image_np, 9, 75, 75)

    # sharpened_image = cv2.filter2D(bilateral_filtered_image, -1, np.array([[-1, -1, -1],
    #                                                                     [-1,  9, -1],
    #                                                                     [-1, -1, -1]]))


    # gray_image = cv2.cvtColor(sharpened_image, cv2.COLOR_BGR2GRAY)
    # equalized_image = cv2.equalizeHist(gray_image)
    # enhanced_image = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2BGR)


    # kernel = np.ones((5, 5), np.uint8)
    # opening_image = cv2.morphologyEx(enhanced_image, cv2.MORPH_OPEN, kernel)



    inputs = processor(images=sharpened_image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)

    class_queries_logits = outputs.class_queries_logits
    masks_queries_logits = outputs.masks_queries_logits

    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

    # rail => 12, person => 19, Pedestrian Area => 11, unlabeled => -1
    ret = {}
    # Rail
    indices = torch.nonzero(predicted_semantic_map == 12, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list = reversed_indices.tolist()
    reversed_indices_list = sorted(reversed_indices_list)
    # indices_set_rail = set(reversed_indices_list)

    # all_points_rail = set(ground_truth_dict[input_name]['Rail'])

    list_of_lists = ground_truth_dict[input_name]['Rail']
    all_points_rail =  set([tuple(lst) for lst in list_of_lists])

    cnt = 0
    for i in reversed_indices_list:
        if (i[1], i[0]) in all_points_rail:
            cnt += 1
    image.close()
    iou = cnt / (len(all_points_rail) +len(reversed_indices_list) - cnt)
    ret["Rail"] = iou

    # Person
    indices = torch.nonzero(predicted_semantic_map == 19, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list = reversed_indices.tolist()
    reversed_indices_list = sorted(reversed_indices_list)
    # indices_set_person = set(reversed_indices_list)

    # all_points_person = set(ground_truth_dict[input_name]['Person'])
    list_of_lists = ground_truth_dict[input_name]['Person']
    all_points_person =  set([tuple(lst) for lst in list_of_lists])
    cnt = 0
    for i in reversed_indices_list:
        if (i[1], i[0]) in all_points_person:
            cnt += 1
    image.close()
    if cnt == 0:
        ret["Person"] = np.NaN
    else:
        iou = cnt / (len(all_points_person) +len(reversed_indices_list) - cnt)
        ret["Person"] = iou

    # Crossing
    indices = torch.nonzero(predicted_semantic_map == 11, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list = reversed_indices.tolist()
    reversed_indices_list = sorted(reversed_indices_list)
    indices = torch.nonzero(predicted_semantic_map == 13, as_tuple=False)
    reversed_indices = indices[:, [1, 0]]
    reversed_indices_list2 = reversed_indices.tolist()
    reversed_indices_list2 = sorted(reversed_indices_list2)
    reversed_indices_list = reversed_indices_list + reversed_indices_list2
    # indices_set_crossing = set(reversed_indices_list)

    # print(ground_truth_dict[input_name]['Crossing'])
    all_points = [tuple(lst) for lst in ground_truth_dict[input_name]['Crossing']]
    all_points_crossing = set(all_points)

    cnt = 0
    for i in reversed_indices_list:
        if (i[1], i[0]) in all_points_crossing:
            cnt += 1
    image.close()
    if cnt == 0:
        ret["Crossing"] = np.NaN
    else:
        iou = cnt / (len(all_points_crossing) +len(reversed_indices_list) - cnt)
        ret["Crossing"] = iou

    return ret

rail_ious = []
# ground_truth_dict = {}
for i in list(ground_truth_dict.keys()):
    print('image: ', i)
    ret = val_blurred(i)
    print("IOU for Rail class", str(round(ret['Rail'] * 100, 2)) + "%")
    rail_ious.append(ret['Rail'])
    if not np.isnan(ret['Person']):
        print("IOU for Person class", str(round(ret['Person'] * 100, 2)) + "%")
    if not np.isnan(ret['Crossing']):
        print("IOU for Crossing class", str(round(ret['Crossing'] * 100, 2)) + "%")
    # print("IOU for Background", str(round(ret['Background'] * 100, 2)) + "%")
    print("===============")

np.average(np.array(rail_ious))



"""## Edge Detection"""

import cv2
import numpy as np
import requests
import torch
from PIL import Image
from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
import json
import torch
from torch import nn
import matplotlib.pyplot as plt
from matplotlib.path import Path
import numpy as np
import os
processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-large-mapillary-vistas-semantic")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-large-mapillary-vistas-semantic")

image2 = cv2.imread("rs19_val/jpgs/rs19_val/" +  "rs03895" +".jpg")
image = Image.open("rs19_val/jpgs/rs19_val/" +  "rs03895" +".jpg")


gray_image = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)
blurred_image = cv2.GaussianBlur(gray_image, (11,11), 0)
sharpened_image = cv2.filter2D(blurred_image, -1, np.array([[-1, -1, -1],
                                                                      [-1,  9, -1],
                                                                      [-1, -1, -1]]))


equalized_image = cv2.equalizeHist(gray_image)
# enhanced_image = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2BGR)


input_image = np.zeros((1080, 1920), dtype=np.uint8)

inputs = processor(images=image, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

class_queries_logits = outputs.class_queries_logits
masks_queries_logits = outputs.masks_queries_logits
predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

# rail_mask = (predicted_semantic_map == 12).float()
rail_indices = torch.nonzero(predicted_semantic_map == 12, as_tuple=False)


for i in rail_indices:
    input_image[i[0], i[1]] = blurred_image[i[0], i[1]]



#   canny edge detection
edges = cv2.Canny(input_image, 50, 150)


contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

straight_contours = []
for contour in contours:
    epsilon = 0.01 * cv2.arcLength(contour, True)
    approx = cv2.approxPolyDP(contour, epsilon, True)
    if len(approx) > 12:
        continue
    straight_contours.append(approx)


for contour in contours:
    contour_length = cv2.arcLength(contour, closed=True)

    if contour_length > 200:
        print(contour)
        cv2.drawContours(image2, [contour], -1, (0, 255, 0), 2)

cv2.imshow('Edges with Length > 200', image2)
cv2.waitKey(0)
cv2.destroyAllWindows()

import cv2

# Read the image
image = cv2.imread("rs19_val/jpgs/rs19_val/" +  "rs03179" +".jpg")

# Convert the image to grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
blurred_image = cv2.GaussianBlur(gray_image, (7,7), 0)

# Apply Canny edge detection
edges = cv2.Canny(blurred_image, 50, 150)  # Adjust the thresholds as needed

# Display the original image and the detected edges
cv2.imshow('Original Image', image)
# cv2.imshow('Detected Edges', edges)
cv2.waitKey(0)
cv2.destroyAllWindows()







# from datasets import load_dataset
# hf_dataset_identifier = "segments/sidewalk-semantic"
# ds = load_dataset(hf_dataset_identifier)

"""## Fine Tune"""

# Unlabeled => 65
# Person => 19
# Road => 13
# Rail = > 12

id2label = {65: 'unlabeled', 19: 'person', 'Road': 13, 'Rail': 12}
with open('id2label.json', 'w') as fp:
    json.dump(id2label, fp)


image_name = images_available_li[0]
image_label = torch.full((1080, 1920), 65)

for k, v in ground_truth_dict[image_name].items():
    for points in v:
        if k == "Person":
            image_label[points[0],points[1]] = 19
        elif k == "Road":
            image_label[points[0],points[1]] = 13
        elif k == "Rail":
            image_label[points[0],points[1]] = 12

# Prepare the building of dataset for segmentation transformer
from PIL import Image

for image_name in list(ground_truth_dict.keys())[5:]:
    image_label = torch.full((1080, 1920), 65)
    for k, v in ground_truth_dict[image_name].items():
            for points in v:
                if k == "Person":
                    image_label[points[0],points[1]] = 19
                elif k == "Road":
                    image_label[points[0],points[1]] = 13
                elif k == "Rail":
                    image_label[points[0],points[1]] = 12
            gray_image = np.array(image_label, dtype=np.uint8)
            gray_image = Image.fromarray(gray_image, mode='L')
            gray_image.save("train/" + image_name + ".png")
            # gray_image_pil.show()

# Create dataset
# https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation#note-on-custom-data
from datasets import Dataset, DatasetDict, Image

def create_dataset(image_paths, label_paths):
    dataset = Dataset.from_dict({"pixel_values": sorted(image_paths),
                                "label": sorted(label_paths)})
    dataset = dataset.cast_column("pixel_values", Image())
    dataset = dataset.cast_column("label", Image())

    return dataset

image_paths_train = []
label_paths_train = []
image_paths_validation = []
label_paths_validation = []
train = list(ground_truth_dict.keys())[:15]
for i in train:
  image_paths_train.append("rs19_val/jpgs/rs19_val/" + i+".jpg")
  label_paths_train.append("train" + i + ".png")

vals = list(ground_truth_dict.keys())[5:]
for i in vals:
  image_paths_validation.append("rs19_val/jpgs/rs19_val/" + i+".jpg")
  label_paths_validation.append("val" + i + ".png")

train_dataset = create_dataset(image_paths_train, label_paths_train)
validation_dataset = create_dataset(image_paths_validation, label_paths_validation)


dataset = DatasetDict({
    "train": train_dataset,
    "validation": validation_dataset,
  }
)

hf_dataset_identifier = f"carolsss/rs19"

ds = dataset.shuffle(seed=1)
ds = ds["train"].train_test_split(test_size=0.2)
train_ds = ds["train"]
test_ds = ds["test"]

# Cite from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/Fine-tuning/Fine_tuning_MaskFormerForInstanceSegmentation_on_semantic_sidewalk.ipynb
import numpy as np
from torch.utils.data import Dataset

class ImageSegmentationDataset(Dataset):
    """Image segmentation dataset."""

    def __init__(self, dataset, transform):
        """
        Args:
            dataset
        """
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        original_image = np.array(self.dataset[idx]['pixel_values'])
        original_segmentation_map = np.array(self.dataset[idx]['label'])

        transformed = self.transform(image=original_image, mask=original_segmentation_map)
        image, segmentation_map = transformed['image'], transformed['mask']

        # convert to C, H, W
        image = image.transpose(2,0,1)

        return image, segmentation_map, original_image, original_segmentation_map

# !pip install albumentations --force-reinstall
# Cite from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/Fine-tuning/Fine_tuning_MaskFormerForInstanceSegmentation_on_semantic_sidewalk.ipynb

import albumentations as A

ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255
ADE_STD = np.array([58.395, 57.120, 57.375]) / 255

train_transform = A.Compose([
    A.LongestMaxSize(max_size=1333),
    A.RandomCrop(width=512, height=512),
    A.HorizontalFlip(p=0.5),
    A.Normalize(mean=ADE_MEAN, std=ADE_STD),
])

test_transform = A.Compose([
    A.Resize(width=512, height=512),
    A.Normalize(mean=ADE_MEAN, std=ADE_STD),

])

train_dataset = ImageSegmentationDataset(train_ds, transform=train_transform)
test_dataset = ImageSegmentationDataset(test_ds, transform=test_transform)

from torch.utils.data import DataLoader

def collate_fn(batch):
    inputs = list(zip(*batch))
    images = inputs[0]
    segmentation_maps = inputs[1]
    batch = preprocessor(
        images,
        segmentation_maps=segmentation_maps,
        return_tensors="pt",
    )

    batch["original_images"] = inputs[2]
    batch["original_segmentation_maps"] = inputs[3]

    return batch

train_dataloader = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)
test_dataloader = DataLoader(test_ds, batch_size=2, shuffle=False, collate_fn=collate_fn)
outputs = model(batch["pixel_values"],
                class_labels=batch["class_labels"],
                mask_labels=batch["mask_labels"])

# Fine Tune
num_epochs = 5
from transformers import AdamW, get_scheduler

# Define the optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
scheduler = get_scheduler("linear", optimizer, num_warmup_steps=0, num_training_steps=num_epochs)

for epoch in range(num_epochs):
    model.train()

    for image_name in images_available_li[:5]:
        image_path = os.path.join(data_dir, image_name)
        image_path = image_path + '.jpg'
        image = Image.open(image_path).convert("RGB")
        inputs = processor(image, return_tensors="pt")
        print(inputs)

        # if image_name in class_labels:
        #     labels = class_labels[image_name]
        #     labels = torch.tensor(labels)
        # else:
        #     labels = None

        optimizer.zero_grad()
        outputs = model(**inputs, labels=image_label)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    # Update learning rate
    scheduler.step()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

# Save the fine-tuned model
model.save_pretrained("fine_tuned_model")



# def val(input_name):
#     image = Image.open("rs19_val/jpgs/rs19_val/" + input_name+".jpg")
#     # image = images[0]
#     inputs = processor(images=image, return_tensors="pt")
#     with torch.no_grad():
#         outputs = model(**inputs)

#     class_queries_logits = outputs.class_queries_logits
#     masks_queries_logits = outputs.masks_queries_logits

#     # Pass them to processor for postprocessing
#     predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

#     indices = torch.nonzero(predicted_semantic_map == 12, as_tuple=False)
#     plt.figure(figsize=(10, 5))
#     plt.imshow(predicted_semantic_map)
#     reversed_indices = indices[:, [1, 0]]
#     reversed_indices_list = reversed_indices.tolist()
#     reversed_indices_list = sorted(reversed_indices_list)

#     try:
#         with open("rs19_val/jsons/rs19_val/" + input_name + ".json", "r") as json_file:
#             ground_truth = json.load(json_file)
#     except FileNotFoundError:
#         return -1,-1
#     all_objects = ground_truth['objects']
#     all_points = []
#     for i in all_objects:
#         if i['label'] == 'rail':
#             if 'polyline-pair' in i.keys():
#                 all_points += i['polyline-pair']
#             elif 'boundingbox' in i.keys():
#                 all_points += bounding_box_coordinates(bounding_box)
#             elif 'polyline' in i.keys():
#                 all_points += i['polyline']
#     # all_points = set(all_points)
#     flattened_list = [item for sublist in all_points for item in sublist]
#     cnt = 0
#     for i in reversed_indices_list:
#         if i in flattened_list:
#             cnt += 1
#     image.close()
#     iou = cnt / (len(flattened_list) +len(reversed_indices_list) - cnt)
#     return cnt, iou

# def val(input_name):
#     image = Image.open("rs19_val/jpgs/rs19_val/" + input_name+".jpg")
#     # image = images[0]
#     inputs = processor(images=image, return_tensors="pt")
#     with torch.no_grad():
#         outputs = model(**inputs)

#     class_queries_logits = outputs.class_queries_logits
#     masks_queries_logits = outputs.masks_queries_logits

#     # Pass them to processor for postprocessing
#     predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]

#     indices = torch.nonzero(predicted_semantic_map == 12, as_tuple=False)
#     plt.figure(figsize=(10, 5))
#     plt.imshow(predicted_semantic_map)
#     reversed_indices = indices[:, [1, 0]]
#     reversed_indices_list = reversed_indices.tolist()
#     reversed_indices_list = sorted(reversed_indices_list)

#     try:
#         with open("rs19_val/jsons/rs19_val/" + input_name + ".json", "r") as json_file:
#             ground_truth = json.load(json_file)
#     except FileNotFoundError:
#         return -1,-1
#     all_objects = ground_truth['objects']
#     all_points = []
#     for i in all_objects:
#         if i['label'] == 'rail':
#             if 'polyline-pair' in i.keys():
#                 all_points += poly_pair_coord(i['polyline-pair'])
#             elif 'boundingbox' in i.keys():
#                 all_points += bounding_box_coord(i['boundingbox'])
#             elif 'polyline' in i.keys():
#                 all_points += i['polyline']
#             elif 'polygon' in i.keys():
#                 all_points += polygon_coord(i['polygon'])
#     # all_points = set(all_points)
#     flattened_list = [item for sublist in all_points for item in sublist]
#     cnt = 0
#     for i in reversed_indices_list:
#         if i in flattened_list:
#             cnt += 1
#     image.close()
#     iou = cnt / (len(flattened_list) +len(reversed_indices_list) - cnt)
#     return cnt, iou











# Vistas Data Category

CLASSES = ['Bird', 'Ground Animal', 'Curb', 'Fence', 'Guard Rail', 'Barrier',
               'Wall', 'Bike Lane', 'Crosswalk - Plain', 'Curb Cut', 'Parking', 'Pedestrian Area',
               'Rail Track', 'Road', 'Service Lane', 'Sidewalk', 'Bridge', 'Building', 'Tunnel',
               'Person', 'Bicyclist', 'Motorcyclist', 'Other Rider', 'Lane Marking - Crosswalk',
               'Lane Marking - General', 'Mountain', 'Sand', 'Sky', 'Snow', 'Terrain', 'Vegetation',
               'Water', 'Banner', 'Bench', 'Bike Rack', 'Billboard', 'Catch Basin', 'CCTV Camera',
               'Fire Hydrant', 'Junction Box', 'Mailbox', 'Manhole', 'Phone Booth', 'Pothole',
               'Street Light', 'Pole', 'Traffic Sign Frame', 'Utility Pole', 'Traffic Light',
               'Traffic Sign (Back)', 'Traffic Sign (Front)', 'Trash Can', 'Bicycle', 'Boat',
               'Bus', 'Car', 'Caravan', 'Motorcycle', 'On Rails', 'Other Vehicle', 'Trailer',
               'Truck', 'Wheeled Slow', 'Car Mount', 'Ego Vehicle', 'Unlabeled']

# CLASSES[12]
index_person = CLASSES.index("Person")
index_pedestrian_area = CLASSES.index("Person")
index_pedestrian_area = CLASSES.index("Rail Track")
index_pedestrian_area
# Unlabeled => 65
# Person => 19
# Road => 13
# Rail = > 12

